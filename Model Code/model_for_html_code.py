# -*- coding: utf-8 -*-
"""model for html code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FNNel0cDhJqOy-YKvZ4QYyckhRDoWfBR
"""

import numpy as np
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/My Drive/ProjectTerm_1188007

import io
df = pd.read_csv('PhiUSIIL_Phishing_URL_Dataset.csv')

feature_count = len(df.columns)
print(f"Number of features: {feature_count}")

df['label'].value_counts()

import matplotlib.pyplot as plt
df['label'].value_counts().plot(kind='bar', color='skyblue')
plt.gca().spines[['top', 'right',]].set_visible(False)
plt.xlabel('Label')
plt.ylabel('Count')
plt.title('Distribution of Label Feature')
plt.show()

print(list(df.columns))

df['TitleLength'] = df['Title'].str.len()

df.head()

df['TotalRef'] = df['NoOfSelfRef'] + df['NoOfEmptyRef'] + df['NoOfExternalRef']
df.head()

df.drop(['FILENAME', 'URL', 'Domain','URLLength', 'TLD', 'Title', "URLSimilarityIndex","CharContinuationRate",
    "TLDLegitimateProb","NoOfLettersInURL",
    "LetterRatioInURL",
    "NoOfDegitsInURL",
    "DegitRatioInURL",
    "NoOfOtherSpecialCharsInURL",
    "SpacialCharRatioInURL","NoOfExternalRef","NoOfSelfRef",
    "URLCharProb","IsHTTPS",
    "LineOfCode",
    "LargestLineLength","DomainTitleMatchScore",
    "URLTitleMatchScore", "DomainLength",
    "IsDomainIP",
    "TLDLength",
    "NoOfSubDomain",
    "HasObfuscation",
    "NoOfObfuscatedChar",
    "ObfuscationRatio",
    "NoOfEqualsInURL",
    "NoOfQMarkInURL",
    "NoOfAmpersandInURL",
    "Robots",
    "NoOfURLRedirect",
    "NoOfSelfRedirect",
    "HasExternalFormSubmit",
    "HasPasswordField",
    "Bank",
    "Pay",
    "Crypto",], axis=1, inplace=True)

print(list(df.columns))

df.shape

"""Data Preparation"""

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
X = df.drop('label', axis=1)
y = df['label']

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier


modelRF = RandomForestClassifier()
modelRF.fit(X_train_scaled, y_train)

y_pred = modelRF.predict(X_test_scaled)

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

print('Confusion Matrix:')
print(confusion_matrix(y_test, y_pred))

print('Classification Report:')
print(classification_report(y_test, y_pred))

# Predicting on validation set
y_val_pred = modelRF.predict(X_val_scaled)

# Evaluating the model on the validation set
print("Validation Accuracy:", accuracy_score(y_val, y_val_pred))
print("\nClassification Report:\n", classification_report(y_val, y_val_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_val, y_val_pred))

#If want to evaluate on the test set too:
y_test_pred = modelRF.predict(X_test_scaled)
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nTest Classification Report:\n", classification_report(y_test, y_test_pred))
print("\nTest Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))

"""XGBoost Validation Accuracy"""

import xgboost as xgb

xgmodel01 = xgb.XGBClassifier(learning_rate=0.1, n_estimators=100, random_state=42)
xgmodel01.fit(X_train_scaled, y_train)

# Make predictions
y_pred = xgmodel01.predict(X_test_scaled)

# Evaluating the model
xgscore = accuracy_score(y_test, y_pred)
print("Accuracy:", xgscore)

print('Confusion Matrix:')
print(confusion_matrix(y_test, y_pred))
print('Classification Report:')
print(classification_report(y_test, y_pred))

# Predict on validation set
y_val_pred = xgmodel01.predict(X_val_scaled)

# Evaluating the model on the validation set
print("Validation Accuracy:", accuracy_score(y_val, y_val_pred))
print("\nClassification Report:\n", classification_report(y_val, y_val_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_val, y_val_pred))

# If want to evaluate on the test set too:
y_test_pred = xgmodel01.predict(X_test_scaled)
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nTest Classification Report:\n", classification_report(y_test, y_test_pred))
print("\nTest Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))

from sklearn.linear_model import LogisticRegression
modelLR = LogisticRegression(solver='lbfgs', max_iter=2000, random_state=42)
modelLR.fit(X_train_scaled, y_train)
y_pred = modelLR.predict(X_test_scaled)
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
print('Confusion Matrix:')
print(confusion_matrix(y_test, y_pred))
print('Classification Report:')
print(classification_report(y_test, y_pred))

# Predicting on validation set
y_val_pred = modelLR.predict(X_val_scaled)

# Evaluating the model on the validation set
print("Validation Accuracy:", accuracy_score(y_val, y_val_pred))
print("\nClassification Report:\n", classification_report(y_val, y_val_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_val, y_val_pred))

# If want to evaluate on the test set too:
y_test_pred = modelLR.predict(X_test_scaled)
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nTest Classification Report:\n", classification_report(y_test, y_test_pred))
print("\nTest Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))

from sklearn.svm import SVC
modelSVM = SVC(kernel='linear', C=1.0)
modelSVM.fit(X_train_scaled, y_train)
y_pred = modelSVM.predict(X_test_scaled)
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
print('Confusion Matrix:')
print(confusion_matrix(y_test, y_pred))
print('Classification Report:')
print(classification_report(y_test, y_pred))

# Predicting on validation set
y_val_pred = modelSVM.predict(X_val_scaled)

# Evaluating the model on the validation set
print("Validation Accuracy:", accuracy_score(y_val, y_val_pred))
print("\nClassification Report:\n", classification_report(y_val, y_val_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_val, y_val_pred))

# If want to evaluate on the test set too:
y_test_pred = modelSVM.predict(X_test_scaled)
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nTest Classification Report:\n", classification_report(y_test, y_test_pred))
print("\nTest Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))

"""Deep Learning Model"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Define the model
modelDL = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    layers.Dropout(0.2),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(1, activation='sigmoid')
])

# Compile the model
modelDL.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
history = modelDL.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_val_scaled, y_val))

# Evaluate the model
loss, accuracy = modelDL.evaluate(X_test_scaled, y_test, verbose=0)
print('Test accuracy:', accuracy)

# Make predictions
y_pred_prob = modelDL.predict(X_test_scaled)
y_pred = (y_pred_prob > 0.5).astype(int)

# Print classification report and confusion matrix
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

print(f'Accuracy: {accuracy:.2f}')

"""Saving the  models"""

import joblib

# Save the models using joblib
joblib.dump(modelSVM, 'SVM_forHTML.pkl')
joblib.dump(modelRF, 'RF_forHTML.pkl')
joblib.dump(modelLR, 'LR_forHTML.pkl')
joblib.dump(xgmodel01, 'XG_forHTML.pkl')